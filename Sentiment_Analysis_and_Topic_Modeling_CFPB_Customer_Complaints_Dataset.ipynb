{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htwX1zyax6HL"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "\n",
        "The following code takes public complaints data sent to the Consumer Financial Protection Bureau (CFPB) and performs a variety of sentiment analysis methods on it in an attempt to quantify the amount of positive and/or negative sentiment customers have toward different credit card providers in the United States.  Some of these sentiment scores were merged with another CFPB 'dataset containing information on more than 650 available US credit cards, assigning these sentiment scores to each card. this merged dataset was then used to build the credit card search tool contained in the other file in thie github repository. the datasets mentioned can be found at the links provided below.\n",
        "\n",
        "- Credit Card List: https://www.consumerfinance.gov/data-research/credit-card-data/terms-credit-card-plans-survey/\n",
        "\n",
        "- Customer complaints dataset: https://www.consumerfinance.gov/data-research/consumer-complaints/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The project entails creating an unbiased credit card comparison tool. A part of the requirement is teams to \"uncover underlying trends, feelings, themes, and concepts that reveal consumersâ€™ perspectives about different credit cards\" as well as to focus the credit card comparison tool on \"the total cost of credit card ownership.\" The first aspect of the project involves analyzing the consumer complaints dataset. Many approaches are available in this set of notes, some sentiment analysis and topic modeling approaches are discussed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "-p15B9zCxABY",
        "outputId": "9ccd6a18-f49a-4240-a0ea-1cd6fd55bf38"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-74a62d0ebeca>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Now you can read files stored in your Google Drive\n",
        "file_path = '/content/drive/My Drive/complaints.csv'\n",
        "df0 = pd.read_csv(file_path,low_memory=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZssXk9ZQ4Cjw"
      },
      "source": [
        "If you are receiving a warning about mixed data, you can just use\n",
        "[**low_memory: bool, default True**](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).<br>\n",
        "This \"internally processes the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser).\"\n",
        "\n",
        "`df0 = pd.read_csv(file_path,low_memory=False)`\n",
        "\n",
        "I am not going to use this column for analysis so I can ignore it for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If23Kh6_mad6"
      },
      "outputs": [],
      "source": [
        "df0['Product'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wjHfmsTJuEy"
      },
      "source": [
        "## Keep Relevant Data\n",
        "Although the analyses performed on this notebook can be done for all of the dataset, for the sake of time, we will only focus on the relevant subset, credit cards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqHme6-mmY2P"
      },
      "outputs": [],
      "source": [
        "# List of products to keep\n",
        "products_to_keep = ['Credit card or prepaid card', 'Credit card']\n",
        "\n",
        "# Filter the DataFrame to only include the desired products\n",
        "df = df0[df0['Product'].isin(products_to_keep)]\n",
        "#Note you may want to filter the data even further to exclude\n",
        "#Government benefit card                         9070\n",
        "#General-purpose prepaid card                    8532\n",
        "#Gift card                                       1021\n",
        "#Payroll card                                     918\n",
        "#Student prepaid card                              32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJTxv5oKDsWn"
      },
      "outputs": [],
      "source": [
        "df0[df0['Product'] == 'Credit card']['Sub-product'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLot391zWK7"
      },
      "source": [
        "## Explore the Dataset\n",
        "I would like to know:\n",
        "1. What features are there? How many observations?\n",
        "2. What are the different subproduct types, issue, and sub-issue types? How many complaints are there related to each?\n",
        "3. What types of responses companies had for consumers?\n",
        "4. How frequently are consumers complaining about each company?\n",
        "5. How many non-null text comments are there?\n",
        "6. What is the volume of complaints over time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST4WpF6T2VEU"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuIai_E4i1q_"
      },
      "outputs": [],
      "source": [
        "#\"This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.\"\"\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqvvwIwtgt-g"
      },
      "outputs": [],
      "source": [
        "df['Sub-product'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j41JzcGWhx_m"
      },
      "outputs": [],
      "source": [
        "df['Issue'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sgel3kAh2jy"
      },
      "outputs": [],
      "source": [
        "df['Sub-issue'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uMi1Z_ziPSU"
      },
      "outputs": [],
      "source": [
        "df['Company response to consumer'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xrsuB2AiXD1"
      },
      "outputs": [],
      "source": [
        "df['Company'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTqYi6tr2wft"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print('\\n\\nMissing Values\\n',df.isnull().sum(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tUW-fsZk-Cm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the date column is in datetime format\n",
        "df['Date received'] = pd.to_datetime(df['Date received'])\n",
        "\n",
        "# Group by month (or year if you prefer) and count complaints\n",
        "complaints_over_time = df.groupby(df['Date received'].dt.to_period(\"M\")).size()\n",
        "\n",
        "# Convert the period index back to datetime for plotting\n",
        "complaints_over_time.index = complaints_over_time.index.to_timestamp()\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "complaints_over_time.plot(title='Volume of Complaints Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Complaints')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWAg9Njqm2GS"
      },
      "source": [
        "# What is Sentiment Analysis\n",
        "Sentiment analysis is the process of identifying and projectifying the sentiment expressed in a piece of text. It can be used to determine the overall sentiment of a document, opinion (polarity, e.g., negative or positive), emotion (joy, suprise, anger, disgust), and subject of the text.\n",
        "\n",
        "There are many methods of sentiment analysis. Nandwani and Verma (2021) and Wankhade et al. (2022) generally categorize them as lexicon based, machine learning, hybrid, and other approaches. Please see the two images below:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Wankhade et al. (2022)](https://drive.google.com/uc?export=view&id=1T5adBteKDFxI86D1_W2Xsx1F9DKoulal)\n",
        "\n",
        "Figure 4 of Wankhade et al. (2022)\n",
        "\n",
        "\n",
        "\n",
        "![Nandwani and Verma](https://drive.google.com/uc?export=view&id=1XDE1ZU5wC31oMJ866cJxZ81GFiOX28ro)\n",
        "\n",
        "Figure 4 of Nandwani and Verma (2021)\n",
        "\n",
        "Since the complaints data is unlabeled, we have three options: use (1) unsupervised clustering,(2) lexicon or rule based models, or (3) transfer learning.\n",
        "1. Unsupervised clustering may give the best results, but also is the most involved.\n",
        "2. Lexicon or rule based methods such as Valence Aware Dictionary and sEntiment Reasoner (VADER) and TextBlob use predefined sets of rules and lexicons to estimate sentiment directly from text without needing further training. These can be applied directly to unlabeled texts. In addition, lexicon based models has been proven to provide reasonable accuracy. Specifically, VADER has performed well compared to TextBlob for getting sentiment of review (Barai 2024) and tweet data(Singh et al. 2022). For example, Singh et al. (2022) utilize a weak-supervision method (the data is labeled using lexicon based methods and supervised machine learning models are applied). Singh et al. (2022) manually observed 500 instances (text) and found that VADER, TextBlob, and NLTK predicted 427, 415, and 398 instances correctly, respectively. Although it must be noted that the data used was twitter for which VADER was designed.\n",
        "3. unsupervised transfer learning also applies. I have completed this for topic modeling, not for sentiment analysis.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyMMyBP33s1G"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG44L-Kpm5LW"
      },
      "source": [
        "#### Minimal Data Preprocessing (VADER)\n",
        "I preprocess the data for text mining. However, unlike other lexicon based methods, VADER, goes beyond a bag-of-words model and captures the amplifying impact of punctuation, capitalization, intensifier words, and contrastive conjunction. Hence, for the first analysis, I do not perform all of the common preprocessing methods. Specifically, I perform the following:\n",
        "\n",
        "**Common Preprocessing:**\n",
        "1. Remove missing entries\n",
        "2. Spell check*\n",
        "3. Remove leading, trailing spaces\n",
        "\n",
        "In addition to the common preprocessing listed above, complaints data also has new line symbols (\\n), XXs used to deidentify consumer names and dates, {}, $, and numbers.\n",
        "\n",
        "**Preprocessing Specific to Complaints Data:**\n",
        "3. Remove Xs, {}, $, \\n, and numbers.\n",
        "\n",
        "\n",
        "I will perform all the tasks in the specified order, except for spell checking, as I do not wish to correct potential misspellings of 'XXs' in the text.\n",
        "\n",
        "*I did not perform this operations as it took quite a bit of time. I was still able to get meaningful results.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQbJrFSInhOT"
      },
      "outputs": [],
      "source": [
        "#Let's drop missing complaints\n",
        "df = df.dropna(subset=['Consumer complaint narrative'])\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXi6v5GMusSA"
      },
      "source": [
        "I want to see what these comments look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnteHuwruwMA"
      },
      "outputs": [],
      "source": [
        "# Randomly sample 10 comments from the DataFrame\n",
        "sampled_comments = df.sample(n=10, random_state=42)  # Change n to adjust the number of comments\n",
        "\n",
        "# Print each sampled comment\n",
        "for index, row in sampled_comments.iterrows():\n",
        "    print(row['Consumer complaint narrative'])\n",
        "    print('-' * 80)  # Print a separator for better readability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GEvS1GfkNGT"
      },
      "outputs": [],
      "source": [
        "# Count the number of entries where 'Consumer complaint narrative' contains newline characters\n",
        "num_entries_with_newlines = df['Consumer complaint narrative'].str.contains('\\n', na=False).sum() #na=False, means if the string is NaN, it won't be included\n",
        "print(f\"Number of entries with newline characters: {num_entries_with_newlines}\")\n",
        "\n",
        "# Filter to find all entries with newline characters\n",
        "comments_with_newlines = df[df['Consumer complaint narrative'].str.contains('\\n', na=False)]\n",
        "\n",
        "# Randomly sample 10 of these entries\n",
        "sampled_comments = comments_with_newlines.sample(n=10, random_state=42)  # Set a random state for reproducibility\n",
        "\n",
        "# Replace newline characters with a visible marker and print each comment\n",
        "for index, row in sampled_comments.iterrows(): #iterate over each row. iterrows() returns a tuple containing index (unique identifier of row in dataset), row of dataset in pandas df\n",
        "    formatted_comment = row['Consumer complaint narrative'].replace('\\n', '\\\\n') #\\n is not visible in the output, \\\\n makes it visible\n",
        "    print(f\"Sampled Comment {index} with visible newline markers:\")\n",
        "    print(formatted_comment)\n",
        "    print('-' * 80)  # Print a separator for better readability: prints 80 - characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaz16280L-Vg"
      },
      "source": [
        "### Removing Special Characters\n",
        "VADER utilizes punctuation, e.g. !, to amplify the negativeness or positiveness of a word. Hence, we will not remove punctuation marks just yet. However, there are (), {}, [], $, or numbers that do not contribute to the sentiment, I will remove those. In addition, I will also remove the XXs,one letter words, and newline characters.\n",
        "\n",
        "Unfortunately, I wasn't able to complete the spell check because it took too long. However, performing spell checks isn't a common requirement in sentiment analysis or topic modeling. There might be methods to speed up the process, though I haven't explored these in detail. Please feel free to investigate this further if you have the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6eomTtGUMqN"
      },
      "source": [
        "#### REGEX\n",
        "Regular expressions (regex) are a powerful tool for matching patterns within text, allowing you to identify, extract, replace, or split parts of strings based on specific patterns. Regex is used across various programming languages and tools for complex string manipulation tasks. Here's an introduction to some of the fundamental components and concepts of regular expressions:\n",
        "\n",
        "**Basic Components**\n",
        "* **Literals:** These are the simplest form of patterns. Each literal matches itself in the text. For example, the regex cat will match the sequence \"cat\" in any string that contains it.\n",
        "\n",
        "* **Character Classes:** Denoted by square brackets [], these match any one character from a set of characters. For example, [abc] will match any single occurrence of 'a', 'b', or 'c'.\n",
        "\n",
        "* **Negated Character Classes:** By including a caret ^ at the start of a character project, you can negate it. For example, [^abc] matches any character except 'a', 'b', or 'c'.\n",
        "\n",
        "* **Dot .:** A dot matches any single character except newline characters.\n",
        "\n",
        "**Anchors:**\n",
        "\n",
        "* ^ (caret) matches the start of a string.\n",
        "* $ (dollar) matches the end of a string.\n",
        "Quantifiers\n",
        "* \\* (asterisk): Matches 0 or more occurrences of the preceding element.\n",
        "* \\+ (plus): Matches 1 or more of the preceding element.\n",
        "* ? (question mark): Makes the preceding element optional, matching either 0 or 1 times.\n",
        "* {n}: Matches exactly n occurrences of the preceding element.\n",
        "* {n,}: Matches n or more occurrences.\n",
        "* {n,m}: Matches between n and m occurrences, inclusive.\n",
        "\n",
        "**Special Characters and Escape Sequences**\n",
        "* \\\\ (backslash): Used to escape special characters, turning them into literals. For example, \\., \\\\, \\^, etc.\n",
        "* \\d: Matches any digit, equivalent to [0-9].\n",
        "* \\D: Matches any non-digit, equivalent to [^0-9].\n",
        "* \\w: Matches any word character (letters, digits, underscore), equivalent to [a-zA-Z0-9_].\n",
        "* \\W: Matches any non-word character.\n",
        "* \\s: Matches any whitespace character (spaces, tabs, newlines).\n",
        "* \\S: Matches any non-whitespace character.\n",
        "\n",
        "**Grouping and Capturing**\n",
        "\n",
        "Parentheses () are used for grouping parts of a pattern and capturing the content matched by those parts. For example, (abc)+ will match one or more sequences of \"abc\" and remember the last \"abc\" matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EvKh0ZNkjnc"
      },
      "outputs": [],
      "source": [
        "# Install spellchecker\n",
        "!pip install pyspellchecker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMBbQ9gg3NS1"
      },
      "outputs": [],
      "source": [
        "#you may use this code to test a pattern before you apply it.\n",
        "import re\n",
        "\n",
        "# Define the pattern to match words containing at least two consecutive 'X' characters\n",
        "pattern1 = r'\\b\\w*X{2,}\\w*\\b'\n",
        "\n",
        "# Example text\n",
        "text = \"Here are some examples: excellent, exXXon, XXL, foXXy, taxonomy, next, XX/XX/XXXX.\"\n",
        "\n",
        "# Find matches using the regular expression\n",
        "matches = re.findall(pattern1, text)\n",
        "\n",
        "# Print the pattern for clarity\n",
        "print(\"Regex Pattern:\", pattern1)\n",
        "\n",
        "# Print the matches\n",
        "print(\"Matches in the text:\", matches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx8abftQn46N"
      },
      "outputs": [],
      "source": [
        "import re #(17 sec)\n",
        "#from spellchecker import SpellChecker\n",
        "\n",
        "\n",
        "# Initialize the spell checker\n",
        "#spell = SpellChecker()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove words containing multiple 'X's\n",
        "    text = re.sub(r'\\b\\w*X{2,}\\w*\\b', '', text) # \\b: applies to complete words only, \\w*:matches 0 or more characters, X{2,}: matches 2 or more Xs (not x)\n",
        "    # Remove any newline characters\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "    # Remove  curly braces, parentheses\n",
        "    text = re.sub(r'[{}()/$]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\b\\d+\\b', '', text) #\\d+ 1 or more of the proceeding elementt\n",
        "    # Remove one-letter words\n",
        "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
        "    # Collapse multiple spaces into a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    '''# Perform spell check and correct the words\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "    for word in words:\n",
        "        # Check each word, correct it if necessary, and ensure it's not None\n",
        "        corrected_word = spell.correction(word)\n",
        "        if corrected_word is not None:\n",
        "            corrected_words.append(corrected_word)\n",
        "        else:\n",
        "            # If spell.correction returns None, use the original word\n",
        "            corrected_words.append(word)\n",
        "    corrected_text = ' '.join(corrected_words)'''\n",
        "    return text.strip() #removes any trailing spaces\n",
        "\n",
        "# Clean the 'Consumer complaint narrative' column\n",
        "df['cleaned_Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(clean_text)\n",
        "# Apply the cleaning function with progress bar\n",
        "#df['cleaned_Consumer complaint narrative'] = [clean_text(text) for text in tqdm(df['Consumer complaint narrative'], desc=\"Processing Texts\")]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GWw0OBwKwWA"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCX9LOPbLPI3"
      },
      "source": [
        "### Removing Domain Specific Words\n",
        "Bastani et al. (2019) when performing topic modeling on CFPB complaints dataset removed domain specific words such as company and state names to improve results. Although the lexicon-based sentiment methods do not require this, I will still remove these to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ9GZRH9Eqao"
      },
      "outputs": [],
      "source": [
        "# Create a list of company names and state names\n",
        "company_names = df['Company'].unique().tolist() #turns the unique names in Company column to a list\n",
        "for company_name in company_names:\n",
        "  print(company_name)\n",
        "\n",
        "state_names = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming', 'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJTegtrPNa8B"
      },
      "source": [
        "I believe that when most people write their complaints, they tend to avoid using abbreviations like INC., LLC, CORP, CO, etc. For instance, they might write 'Mercury Technologies' instead of 'Mercury Technologies, Inc.' Consequently, I will add the company names without these corporate designations to our list. Additionally, I will randomly sample the comments to identify any company names that still appear and include those in the list as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hN8qyWNFHyR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def process_company_names(names):\n",
        "    # Updated regex pattern to effectively remove corporate designations, including \", THE\"\n",
        "    pattern = re.compile(r'\\s*,?\\s*(Inc\\.|Corporation|Corp\\.|COMPANY|INTERMEDIATE HOLDINGS|LLC|L\\.L\\.C\\.|Co\\.|National Association|The|N\\.A\\.|INC/)\\.?\\s*$', re.IGNORECASE)\n",
        "    #\\s*: leading spaces before or after characters, ,?: 0 or 1 commas preceding (e.g. ,Inc.) For example, \\s*,?\\s* means space before of after optional comma\n",
        "    #\\.?:matches optional period after the word,  \\s*: space after the word\n",
        "    # | means or\n",
        "\n",
        "\n",
        "\n",
        "    # Process each name in the list\n",
        "    processed_names = []\n",
        "    for name in names:\n",
        "        # Remove designations, including any trailing spaces\n",
        "        clean_name = pattern.sub('', name).strip()\n",
        "        # Append original name and cleaned name if they differ\n",
        "        processed_names.append(name)\n",
        "        if clean_name != name:\n",
        "            processed_names.append(clean_name)\n",
        "\n",
        "    return processed_names\n",
        "\n",
        "\n",
        "# Apply the function to the original list\n",
        "updated_company_names = process_company_names(company_names)\n",
        "print(updated_company_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5iZlu6pnNGZ"
      },
      "outputs": [],
      "source": [
        "#Unfortunately, some consumers did not leave spaces between words, e.g., writing RewardsBarclays instead of Reward Barclays. These show up as topics in topic modeling, let's remove them.\n",
        "#I need to add more names to my list of company names. I must detect cases similar to RewardsBarclays.\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Sample DataFrame setup (commented out but can be uncommented for an actual use-case)\n",
        "# df = pd.DataFrame({'cleaned_Consumer complaint narrative': [\"I still get cardBarclays or RewardsBarclay\",\n",
        "#                                                             \"JuniperBarclays and DepotCitibank are troubling.\",\n",
        "#                                                             \"Love my AMEX card.\",\n",
        "#                                                             \"CITIBank services are great but not AMEX.\"]})\n",
        "\n",
        "# In the first iteration of topic modeling, these were the most common company names that showed up. This should be updated for subsequent runs.\n",
        "names = ['Barclay', 'Barclays', 'CITI', 'Citibank', 'AMEX','Discover']\n",
        "\n",
        "# Create a regex pattern to find words containing these names\n",
        "# Adjust the pattern to capture entire words containing the target names\n",
        "extended_names = [r'\\w*' + re.escape(name) + r'\\w*' for name in names] # re.escape(name) escapes regex metacharacters in 'name', making them literal characters, e.g., \"A+B\" becomes \"A\\+B\".\n",
        "\n",
        "pattern = '|'.join(extended_names)\n",
        "print(pattern)\n",
        "\n",
        "# Initialize an empty set to store unique words\n",
        "unique_words = set()\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for text in df['cleaned_Consumer complaint narrative']:\n",
        "    # Find all words containing the specified names\n",
        "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "    # Update the set with found matches\n",
        "    unique_words.update(matches)\n",
        "\n",
        "# Convert the set to a sorted list\n",
        "sorted_unique_words = sorted(unique_words, key=str.lower)\n",
        "\n",
        "# Print the sorted list of unique words\n",
        "print(\"Unique words containing specified company names:\", sorted_unique_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0yim115nRbW"
      },
      "outputs": [],
      "source": [
        "#not all words inthe unique_words list are company names. I manually checked all and only kep company names\n",
        "updated_unique_words= ['AAdvantageCitibank', 'AbeAmex', 'accountCitibank', 'AmEX', 'AmEx', 'AmeX', 'AMEx', 'AMex', 'amex', 'AMEX', 'Amex', 'AmexBluebird', 'Amexbluebird', 'AmexBluebirds', 'AMEXCA', 'AmexCard', 'AMEXchat', 'AMEXDelta', 'AMEXDSNB', 'AMEXFurnisher', 'amexgiftcard', 'Amexgiftcard', 'AmexGiftCard', 'AMEXGitcard', 'AmExp', 'AMEXPRESS', 'AmexReward', 'amexrewardcard', 'Amexrican', 'Amexrixan', 'AmExs', 'Amexs', 'amexs', 'AMEXs', 'AmexServe', 'Amexto', 'amextrave', 'Amextravel', 'amextravel', 'AmexTravel', 'AMEXTRAVEL', 'applicationapplyprospecttermsamex', 'AskAmex', 'AskAMEX', 'AtCiti', 'bankBARCLAYS', 'BankCiti','BankDiscover', 'BARCLAY', 'Barclay', 'BarClay', 'BArclay', 'barclay', 'BarclayCard', 'Barclaycard', 'BARCLAYCARD', 'barclaycard', 'Barclaycards', 'Barclaycardus', 'barclaycardus', 'BarclayCardUS', 'BarclaycardUS', 'BarclaycardUSA', 'Barclaycrd', 'Barclaycredit', 'barclayed', 'Barclayjetblue', 'BarclayJuniper', 'BarclayRCI', 'barclays', 'BARCLAYs', 'Barclays', 'BArclays', 'BARCLAYS', 'Barclaysand', 'BarclaysBank', 'barclaysbankus', 'BARCLAYSBK', 'BarclaysCard', 'Barclayscard', 'barclayscardus', 'BarclaysCarnival', 'BarclaysChoice', 'BarclaysFrontier', 'BarclaysJetBlue', 'BarclaysJetBlueacc', 'BarclaysMastercard', 'Barclayss', 'Barclaysspecifically', 'Barclayssurrendered', 'Barclaysto', 'BarclaysUber', 'BarclaysUS', 'barclaysus', 'Barclaysus', 'BarclaysUSCard', 'BarclaysVisa', 'BarclayUpromise', 'BarclayUS', 'BBCitibank', 'bestbuyCiti', 'BestBuyCiti', 'bestbuyciti', 'BESTBUYCITI', 'BestbuyCiti', 'BestBuycitibank', 'BestBuyCitibank', 'BestbuyCiticard', 'BloomingdalesAMEX', 'BluebirdAmex', 'BluebirdAMEX', 'BuyCITI', 'buyCiti', 'BuyCiti', 'buyciti', 'Buycitibank', 'BuyCitibank', 'buycitibank', 'buyCitibank', 'BuyCitibankCBNA', 'BuyCiticard', 'BuyCitiGroup', 'BYDISCOVER', 'cardAmex', 'CARDBARCLAYS', 'cardBarclays', 'CardBarclaysUS', 'CardCiti', 'CardCitibank','CardDiscover', 'cardscardamex', 'CardsCitibank', 'cardscreditcardsciti', 'CBNACiti', 'CFPBCiti', 'CitbankCiticards', 'Citi', 'CitI', 'CIti', 'citi', 'CITI', 'cITI', 'CITi', 'CiTi', 'CiTI', 'CITI002', 'Citi_Account_Mail', 'Citi_Approval_Email', 'Citi_Invitation_Email', 'CITIAA', 'CitiAAAdvantage', 'CitiAadavtage', 'citiAadvantage', 'CitiAadvantage', 'CitiAAdvantage', 'CitiAADvantagePlatinum', 'CitiaBank', 'CitiAdvantage', 'CITIANK', 'CITIARD', 'Citib', 'CitiBa', 'citiback', 'Citiback', 'Citibak', 'citibak', 'CitiBaks', 'Citiban', 'CItiBanik', 'citibank', 'CiTiBank', 'CITIbank', 'CitibanK', 'Citibank', 'CItiBank', 'CitiBank', 'CITIBANK', 'CitiBANK', 'CItibank', 'CITIBank', 'CitibankAAadvantage', 'CitibankAACBNA', 'CitibankAAdvantage', 'CitiBankBest', 'CitibankBest', 'CitibankBestBuy', 'CitiBankBestbuy', 'CitibankBestbuy', 'citibankbestbuy', 'CitibankCards_US', 'CitibankCBNA', 'CitibankCiti', 'CITIBANKCITICARD', 'CitiBankCiticard', 'CitibankCiticards', 'CitiBankCitiCards', 'CitibankCitigroup', 'CitiBankCommercialCardsInvestigations', 'CitibankCostco', 'Citibankcredit', 'CitibankCredit', 'citibankhome', 'CitibankHome', 'Citibankhomedepot', 'Citibanki', 'CitibankMacy', 'CitibankMacys', 'citibankMACYS', 'CitibankMasterCard', 'Citibanknot', 'citibankonline', 'citibankrequested', 'Citibanks', 'CitiBanks', 'citibanks', 'CITIBANKs', 'CitibankSears', 'CitiBankSears', 'CitibankSummary', 'CitibankThey', 'CitibankVisa', 'CITIBANL', 'Citibbank', 'citibest', 'CitiBest', 'CitiBestBuy', 'citibestbuy', 'CitiBusines', 'Citibusiness', 'citibusiness', 'citiBusiness', 'CitiBusiness', 'CitiBusinessAAvantagePlatinum', 'citic', 'Citic', 'Citicank', 'CItiCard', 'CITIcard', 'citicard', 'Citicard', 'CITICARD', 'CITICard', 'CitiCard', 'CIticard', 'CitiCardbank', 'Citicardbank', 'CITICARDCBNA', 'CitiCardCitiBank', 'CiticardCitibank', 'CiticardCitigroup', 'CITICARDCITIGROUP', 'CitiCards', 'Citicards', 'citicards', 'CITICARDS', 'CiticardsBB', 'CiticardsCitibank', 'CiticardsVISA', 'Citicardwas', 'CitiCbna', 'CitiCBNA', 'citiciti', 'CitiCo', 'CitiConcierge', 'Citicorp', 'CitiCorp', 'Citicorps', 'CITICOSTCO', 'citiCOSTCO', 'CITICostco', 'CitiCostco', 'citiCostco', 'CITICRDS', 'Citicredit', 'CITICredit', 'citicredit', 'citidisputes', 'CitiDouble', 'citied', 'Citiens', 'citiens', 'CITIFINANCIAL', 'CitiFinancial', 'CitifinancialCitibank', 'Citiflex', 'CitiFraud', 'CitiGold', 'Citigold', 'Citigoup', 'Citigroup', 'citigroup', 'CitiGroup', 'CITIGROUP', 'Citigroups', 'Citihealth', 'citihealth', 'Citihhealth', 'CitiHome', 'CitiI', 'Citiibank', 'citiibank', 'CitiiCards', 'citiicards', 'CITIiscurrentlyinviolationoftheirowntermsandguarantees', 'CITIMastercardShell', 'CitiMC', 'CITIMORTGAGE', 'CITING', 'Citing', 'citing', 'CitiPhone', 'CitiPremier', 'Citiprice', 'citipricerewind', 'CitiRecovery', 'citiretailservices', 'CitiRewards', 'Citiright', 'citis', 'Citis', 'CITIs', 'CitiSears', 'citiSears', 'Citistars', 'citit', 'Cititbank', 'Cititcard', 'CitiThank', 'Citithey', 'CITIto', 'citito', 'CitiVisa', 'citivisa', 'CITIWAYFAIR', 'CitizensOne', 'comamex', 'comblogciti', 'comciti', 'comcitiaboutdataciti_commitment_summary', 'comciticovid', 'comfinanceciticard', 'comselectciti', 'comstatusciti', 'CostcoCiti', 'COSTCOCitibank', 'creditCiti', 'customerCiticards', 'customersCiti','contactDiscovery', 'Discover', 'DIscover', 'DISCOVER', 'DIScover', 'discover','DISCOVERBANK', 'discoverbank', 'DiscoverBankChargeDispute', 'DiscoverBankGeneralCorrespondence', 'DISCOVERCARD', 'DIscoverCard', 'discovercard', 'DiscoverCArd', 'DiscoverCard', 'Discovercard', 'DiscoverCards', 'Discovercc', 'discoverd', 'DISCOVERE', 'discovere','discovering', 'DISCOVERING', 'Discoverist', 'DiscoverIT', 'DISCOVERIT', 'DISCOVERPAYMENT', 'DISCOVERPAYMENTPROTECTION', 'DiscoverPaymentProtection','DeltaAmex', 'DepotCITI', 'DepotCiti', 'DepotCitibank', 'DepotCitiBank', 'DepotCITIBANK', 'DepotCitigroup', 'eliciting', 'emailToCiti', 'ExpressAMEX', 'expressamex', 'ExpressCitibank', 'FooterCiti', 'fromCiti', 'FrontierBarclay', 'FrontierBarclays', 'FrustratedCITIBANKcustomer', 'heamex', 'IhaveacreditcardwithCitibank', 'IhavespentcountlesshoursonthephonewithCITItryingtoresolvethedispute', 'IreceivedaletterfromCITIdetailsintheattachedinformingmethattheydidnotconsidermydisputetobefraudulentactivityandtherefore', 'IspokewithafraudspecialistatCITIfor7minuteswherewewereabletoidentifytheaforementionedthreechargesthatwereNOTauthorizednormadebyme', 'jetbluebarclay', 'JetBlueBarclays', 'JetblueBarclays', 'JuniperBarclays', 'MacyCITI', 'MacyCiti', 'MacysCiti', 'MACYSCitibank', 'MACYSCITIBANK', 'MacysCitibank', 'mastercardbarclays', 'MastercardCiti', 'MasterCardCiti', 'membershipciti', 'moreCiticards', 'myAmex', 'myCITI', 'NavyBarclay','NameDISCOVER', 'nonCitibank', 'PremierCiti','prepaidDiscover', 'RCIBarclays', 'RewardsBarclay', 'sciti', 'sCiti', 'sCitibank', 'scitibank', 'sCiticorp', 'SEARSCITI', 'SearsCiti', 'SearsCitiBank', 'SearsCitibank', 'SearsCiticard', 'ServeAmex', 'serviceamex', 'ServicesBarclay', 'ServicesCitibank', 'shellcitibank', 'ShellCitibank', 'ShellCitigroup', 'tBarclays', 'TCitibank', 'TheyCiti', 'three3fraudulentchargesweremadetomyCITIBankAAdvantageExecutiveMasterCardatthefollowinglocations', 'toshowthatBarclay', 'tSearsCiti', 'UberBarclays', 'UNIVERSALCITI', 'UnvlCiti', 'VilaAmex', 'VilasAmexs', 'VisaBarclays', 'VisaCitiBank', 'WayfairCitibank', 'wBarclays', 'wCiti', 'whichCITI', 'withBarclays', 'WITHCITI', 'withCitiCard','wDiscover', 'youCiti']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V3cR7tsHlvh"
      },
      "outputs": [],
      "source": [
        "# New company names to add\n",
        "new_company_names = [\n",
        "    \"Wells Fargo\", \"Bofa\", \"B of A\", \"Transunion\", \"CITI\" ,\n",
        "    \"PNC\", \"Quick Silver Capital One\",\"Citi Bank\",\n",
        "    \"Costco\", \"TD\", \"Chase\", \"Capital One\", \"US Bank\", \"Bank\",\n",
        "    \"American Express\",\"Discover card\",\"Citibank\",\"Citicard\",\n",
        "    \"Barclay\",\"Barclays\",\"Barclaycard\",\"Paypal\",\"synchrony\",\n",
        "    \"Apple\",\"Goldman Sachs\",\"Goldman\",\"Sachs\",\"Amex\",\"Experian\",\"Discover\"\n",
        "    \"BOA\",\"CFPB\"\n",
        "]\n",
        "\n",
        "updated_company_names=new_company_names+updated_unique_words\n",
        "# Adding new names to the existing list\n",
        "company_names.extend(updated_company_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPOGsREf-WzQ"
      },
      "source": [
        "#### PARALLEL PROCESSING AND DASK\n",
        "Parallel processing refers to the technique of running multiple processes simultaneously on different processors in the same computer, or across multiple computers in a network.\n",
        "\n",
        "**Parallel Processing in Google Colab**\n",
        "\n",
        "- **CPUs**:\n",
        "  Google Colab provides virtual machines with typically 2 CPU cores. Parallel processing can be achieved using Pythonâ€™s `multiprocessing` library or other libraries like `joblib` for tasks across multiple cores.\n",
        "\n",
        "- **GPUs and TPUs**:\n",
        "  For demanding tasks, particularly in machine learning, Colab offers optional access to NVIDIA GPUs and Google TPUs. These resources support highly parallel operations and greatly enhance the speed of compatible processes, such as training deep learning models.\n",
        "\n",
        "- **Dask** breaks down complex tasks into smaller pieces that can be executed concurrently. It integrates well with existing Python libraries like NumPy, pandas, and scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyw-1DpYTdFp"
      },
      "outputs": [],
      "source": [
        "#finding and removing these names is time consuming.\n",
        "#Hence, I will use the parallel processing library, dask. It reduces the time to 5 minutes.\n",
        "!pip install dask[complete]  # includes pandas-like dask dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7TBFtQN9F4-"
      },
      "outputs": [],
      "source": [
        "#Just checking that the regex pattern created actualy catches the company names.\n",
        "#pattern = r'\\b(' + '|'.join([re.escape(name).replace(r'\\ ', r'\\s+') for name in company_names]) + r')\\b'\n",
        "escaped_names = [re.escape(name) for name in company_names]\n",
        "pattern = r'\\b(' + '|'.join(escaped_names) + r')\\b'\n",
        "print(pattern)\n",
        "# Manually check this pattern against a sample string\n",
        "matches=re.findall(pattern, \"We opened credit card through Costco with Citi Bank, which we used for vacation.\", flags=re.IGNORECASE)\n",
        "# Print matches to see what was found by the regex\n",
        "print(\"Matches found:\", matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7zRbYYXTvZh"
      },
      "outputs": [],
      "source": [
        "#21 mins\n",
        "import re\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd ## Dask dataframes are different than Pandas dataframes\n",
        "from dask.diagnostics import ProgressBar\n",
        "\n",
        "def remove_names_from_narratives(df, company_names, state_names):\n",
        "    # Combine company names and state names into one list\n",
        "    all_names = company_names + state_names\n",
        "\n",
        "    # Escape special regex characters in names and create regex pattern\n",
        "    escaped_names = [re.escape(name) for name in all_names]\n",
        "    pattern = r'\\b(' + '|'.join(escaped_names) + r')\\b'\n",
        "\n",
        "    # Function to apply regex and remove names\n",
        "    def remove_names(text):\n",
        "        return re.sub(pattern, '', text, flags=re.IGNORECASE).strip() #ignore case and remove leading and trailing spaces\n",
        "\n",
        "    # Apply the removal function to the DataFrame column using map_partitions\n",
        "    df['NoCompany Complaint'] = df['cleaned_Consumer complaint narrative'].map_partitions(\n",
        "        lambda part: part.apply(remove_names), meta='str') #applies remove_names operation to each partition, meta=str tells Dask the output will be a string\n",
        "    return df\n",
        "\n",
        "# Convert your Pandas DataFrame to a Dask DataFrame\n",
        "dask_df = dd.from_pandas(df, npartitions=4)\n",
        "\n",
        "# Instantiate a progress bar\n",
        "with ProgressBar():\n",
        "    # Clean the narratives using the modified function\n",
        "    dask_df = remove_names_from_narratives(dask_df, company_names, state_names)\n",
        "    # Compute the result back to pandas if needed\n",
        "    result_df = dask_df.compute()\n",
        "\n",
        "print(result_df)\n",
        "df=result_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGvP3HB6ya-j"
      },
      "outputs": [],
      "source": [
        "df=result_df\n",
        "\n",
        "# Randomly sample 5 rows to display\n",
        "sampled_comments = df.sample(n=10, random_state=40)  # Using a fixed random state for reproducibility\n",
        "\n",
        "# Print each sampled cleaned comment\n",
        "for index, row in sampled_comments.iterrows():\n",
        "    formatted_comment = row['NoCompany Complaint']\n",
        "    print(f\"Sampled Cleaned Comment {index}:\")\n",
        "    print(formatted_comment)\n",
        "    print('-' * 80)  # Print a separator for better readability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB-YONPCzLCF"
      },
      "source": [
        "### Preprocessing Data for TextBlob, SentiWordNet, and LDA.\n",
        "\n",
        " Bastani et al. (2019) applied topic model using Latent Dirichlet Allocation (LDA) to the CFPB complaints dataset and achieved better issue categorization than the evident in the dataset. Note that the issues in the dataset are selected by consumers; hence, are expected to be inaccurate.\n",
        "\n",
        "In addition to the minimal data processing for VADER, I will also perform the same data cleaning methods described in  Bastani et al. (2019).  This will give us two different levels of preprocessing: minimally processed and preprocessed. The preprocessing is explained in the paper as follows:\n",
        "1. convert to lowercase,\n",
        "2.  remove special characters including punctuation marks (!%$#*?,/.;'\\) and tokenize.\n",
        "3. remove both the common stopwords and domain specific words.\n",
        "4. Stemming.\n",
        "5. Creating the term-document matrix which is needed for LDA.\n",
        "This process is displayed for a single complaint, or document in the image below.\n",
        "\n",
        "Instead of stemming, I will lemmatize.\n",
        "\n",
        "![Preprocessing.png](https://drive.google.com/uc?export=view&id=1BIDFrbvQrbARaEBa1a6qQxxO_fb_ift0)\n",
        "\n",
        "\n",
        "### Stemming\n",
        "- **Original Word**: \"arguing\"\n",
        "- **Stemmed**: \"argu\"\n",
        "- **Explanation**: Stemming simplifies words to their root form by chopping off endings, often resulting in non-words.\n",
        "\n",
        "### Lemmatization\n",
        "- **Original Word**: \"better\"\n",
        "- **Lemma**: \"good\"\n",
        "- **Explanation**: Lemmatization reduces words to their dictionary form using linguistic analysis, ensuring the output is a valid word.\n",
        "\n",
        "Stemming might be faster and simpler, but it often produces roots that are not actual words, which can introduce noise and ambiguity into the analysis. If you would like, you can create another column with stemmed data and test whether it changes your sentiment and topic model results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5rdSMTE-DJr"
      },
      "outputs": [],
      "source": [
        "#Before cleaning the data, I will create a wordcloud to observe common words.\n",
        "!pip install wordcloud\n",
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(df['cleaned_Consumer complaint narrative'].values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTgAJ2kWBVFN"
      },
      "outputs": [],
      "source": [
        "#35 secs\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "\n",
        "# Load necessary resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define stopwords, tokenizer, lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenizer = RegexpTokenizer(r'\\w+') #\\w: matches any alphanumeric characters (letters and digits), +: 1 or more of the proceeding\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    if pd.isna(text):\n",
        "        return []  # Return an empty list if text is NaN\n",
        "\n",
        "    # Lowercase conversion\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization (removing punctuation and split into unigrams)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Removing numeric tokens and words of length 1\n",
        "    tokens = [token for token in tokens if not token.isnumeric() and len(token) > 1]\n",
        "\n",
        "    # Removing stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Filter out any empty tokens after all transformations\n",
        "    return list(filter(None, tokens))\n",
        "\n",
        "# Apply the normalization function to the specific column\n",
        "df['normalized_narrative'] = df['NoCompany Complaint'].apply(lambda text: normalize_text(text))\n",
        "\n",
        "# Example to show results\n",
        "print(df['normalized_narrative'].head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F28C6UiQD8ab"
      },
      "outputs": [],
      "source": [
        "# Randomly sample 10 rows to display\n",
        "sampled_comments = df.sample(n=10, random_state=40)  # Using a fixed random state for reproducibility\n",
        "\n",
        "# Print each sampled cleaned comment along with VADER scores\n",
        "for index, row in sampled_comments.iterrows():\n",
        "    formatted_comment = row['normalized_narrative']\n",
        "    print(f\"Sampled Cleaned Comment {index}:\")\n",
        "    print(formatted_comment)\n",
        "    print('-' * 80)  # Print a separator for better readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUjKLnJZVkmV"
      },
      "outputs": [],
      "source": [
        "#Let's see how the wordcloud changes after cleaning\n",
        "!pip install wordcloud\n",
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "# Assuming df['normalized_narrative'] contains lists of words as your normalized text data\n",
        "# Convert lists of words back into strings if needed\n",
        "df['text_string'] = df['normalized_narrative'].apply(' '.join)\n",
        "long_string = ','.join(list(df['text_string'].values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4D5AgPh3_Oc"
      },
      "source": [
        "# Sentiment Analysis Application to CFPB Complaints Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTKUvsA9oxko"
      },
      "source": [
        "### VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
        "VADER is a lexicon and rule-based sentiment analysis tool specifically designed to analyze social media text. It is widely recognized for its efficiency in capturing the sentiment expressed in short, informal, and context-rich text, such as tweets, reviews, and comments. Developed by C.J. Hutto and Eric Gilbert, VADER has become a popular choice for researchers and practitioners due to its simplicity and effectiveness.\n",
        "\n",
        "Key Features of VADER\n",
        "1. **Lexicon-Based Approach:** VADER utilizes a predefined list of words (lexicon) that are annotated with sentiment scores. These scores reflect the general sentiment associated with each word, ranging from highly negative to highly positive.\n",
        "\n",
        "2. **Rule-Based Adjustments:** In addition to the lexicon, VADER applies several heuristics and rules to account for the context and intensity of sentiments. For example:\n",
        "  *   **Punctuation:** Exclamation marks (!) and question marks (?) can intensify the sentiment.\n",
        "  *   **Capitalization:** Words in uppercase letters are perceived as more intense.\n",
        "  *   **Degree Modifiers:** Words that amplify (e.g., \"very\") or diminish (e.g., \"slightly\") sentiment are taken into consideration.\n",
        "  *   **Conjunctions:** But, and, or other conjunctions can alter the sentiment of the preceding or following phrases.\n",
        "3. **Sentiment Scores:** VADER provides four sentiment metrics:\n",
        "  * **Negative:** The proportion of the text that conveys negative sentiment.\n",
        "  * **Neutral:** The proportion of the text that conveys neutral sentiment.\n",
        "  * **Positive:** The proportion of the text that conveys positive sentiment.\n",
        "  * **Compound:** A normalized score that represents the overall sentiment of the text, ranging from -1 (most negative) to +1 (most positive).\n",
        "\n",
        "Here is a simple demonstration of VADER's algorithm:\n",
        "\n",
        "Normalized positive, negative and neutral score is found my summing the scores and dividing by the sum of positive, negative and neutral scores.\n",
        "\n",
        "![VADER Sentiment.png](https://drive.google.com/uc?export=view&id=10nw_eHMiDlLPqF-e1sV8jwHAxBm00X0t)\n",
        "\n",
        "Figure adapted from Lee (2021)\n",
        "\n",
        "The compound score considers both negative and positive sentiments.\n",
        "![VADER Compound.png](https://drive.google.com/uc?export=view&id=1txwkzgg6rvOkFzEwOYurd7uBu5WqAbzy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV2A8iWlpN5F"
      },
      "outputs": [],
      "source": [
        "#Appylying to minimally processed data (3 min)\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Create an instance of the Vader sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define a function to get all scores from VADER\n",
        "def get_vader_scores(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores\n",
        "\n",
        "# Apply the function to the 'Consumer complaint narrative' column and store results\n",
        "df['VADER Scores'] = df['NoCompany Complaint'].apply(get_vader_scores)\n",
        "\n",
        "# Now extract each score into its own column\n",
        "df['MVADER Negative'] = df['VADER Scores'].apply(lambda x: x['neg'])\n",
        "df['MVADER Neutral'] = df['VADER Scores'].apply(lambda x: x['neu'])\n",
        "df['MVADER Positive'] = df['VADER Scores'].apply(lambda x: x['pos'])\n",
        "df['MVADER Compound'] = df['VADER Scores'].apply(lambda x: x['compound'])\n",
        "\n",
        "# Optionally, you can drop the 'VADER Scores' column if it's no longer needed\n",
        "# df.drop(columns=['VADER Scores'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwrlf-Ru0Lqg"
      },
      "outputs": [],
      "source": [
        "#Appylying to preprocessed data (1 min)\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Create an instance of the Vader sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define a function to get all scores from VADER\n",
        "def get_vader_scores(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores\n",
        "\n",
        "# Assuming df['normalized_narrative'] contains lists of words as your normalized text data\n",
        "# Convert lists of words back into strings\n",
        "df['text_string'] = df['normalized_narrative'].apply(' '.join)\n",
        "\n",
        "# Apply the function to the 'normalized_narrative' column and store results\n",
        "df['VADER Scores'] = df['text_string'].apply(get_vader_scores)\n",
        "\n",
        "# Now extract each score into its own column\n",
        "df['PVADER Negative'] = df['VADER Scores'].apply(lambda x: x['neg'])\n",
        "df['PVADER Neutral'] = df['VADER Scores'].apply(lambda x: x['neu'])\n",
        "df['PVADER Positive'] = df['VADER Scores'].apply(lambda x: x['pos'])\n",
        "df['PVADER Compound'] = df['VADER Scores'].apply(lambda x: x['compound'])\n",
        "\n",
        "# Optionally, you can drop the 'VADER Scores' column if it's no longer needed\n",
        "# df.drop(columns=['VADER Scores'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SilbHckGtlhU"
      },
      "outputs": [],
      "source": [
        "#Check if VADER Sentiment is added to the dataset.\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns()"
      ],
      "metadata": {
        "id": "968WwyL93rhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThIp2Bzpz6o_"
      },
      "source": [
        "I would like to see the comments in more detail along with their VADER scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWEIS6dKz5vz"
      },
      "outputs": [],
      "source": [
        "# Randomly sample 10 rows to display\n",
        "sampled_comments = df.sample(n=10, random_state=40)  # Using a fixed random state for reproducibility\n",
        "\n",
        "# Print each sampled cleaned comment along with VADER scores\n",
        "for index, row in sampled_comments.iterrows():\n",
        "    formatted_comment = row['normalized_narrative']\n",
        "    print(f\"Sampled Cleaned Comment {index}:\")\n",
        "    print(formatted_comment)\n",
        "    print(\"Minimally processed VADER Negative Score:\", row['MVADER Negative'])\n",
        "    print(\"Minimally processed VADER Neutral Score:\", row['MVADER Neutral'])\n",
        "    print(\"Minimally processed VADER Positive Score:\", row['MVADER Positive'])\n",
        "    print(\"Minimally processed VADER Compound Score:\", row['MVADER Compound'])\n",
        "    print(\"Preprocessed VADER Negative Score:\", row['PVADER Negative'])\n",
        "    print(\"Preprocessed VADER Neutral Score:\", row['PVADER Neutral'])\n",
        "    print(\"Preprocessed VADER Positive Score:\", row['PVADER Positive'])\n",
        "    print(\"Preprocessed VADER Compound Score:\", row['PVADER Compound'])\n",
        "    print('-' * 80)  # Print a separator for better readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAv1KNQUd0i0"
      },
      "source": [
        "I would like to compare the results from VADER using minimally processed and preprocessed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a0t0g7TFn8J"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Ensure the plotting libraries are installed\n",
        "# Note: In a script or interactive session, you only need to install packages once, not every time you run the code.\n",
        "!pip install matplotlib seaborn\n",
        "\n",
        "# Define the subplot grid layout\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))  # 2x2 grid of plots, overall figure size\n",
        "\n",
        "# Define the plot titles\n",
        "titles = [\n",
        "    'MVADER Negative vs PVADER Negative',\n",
        "    'MVADER Neutral vs PVADER Neutral',\n",
        "    'MVADER Positive vs PVADER Positive',\n",
        "    'MVADER Compound vs PVADER Compound'\n",
        "]\n",
        "\n",
        "# Define the axes (x and y pairs) for the plots\n",
        "data_pairs = [\n",
        "    ('MVADER Negative','PVADER Negative'),\n",
        "    ('MVADER Neutral','PVADER Neutral'),\n",
        "    ('MVADER Positive','PVADER Positive'),\n",
        "    ('MVADER Compound','PVADER Compound')\n",
        "]\n",
        "\n",
        "# Loop over the axes and data pairs\n",
        "for ax, (x, y), title in zip(axes.flatten(), data_pairs, titles):\n",
        "    # Scatter plot on specific subplot axis\n",
        "    sns.scatterplot(data=df, x=x, y=y, ax=ax)\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    if df[x].notnull().all() and df[y].notnull().all():  # Ensure no null values\n",
        "        corr_coef = np.corrcoef(df[x], df[y])[0, 1]\n",
        "        # Adding title with correlation\n",
        "        ax.set_title(f'{title}\\nPearson Correlation Coefficient: {corr_coef:.2f}')\n",
        "    else:\n",
        "        ax.set_title(f'{title}\\nData not sufficient for correlation')\n",
        "\n",
        "    # Set x and y labels\n",
        "    ax.set_xlabel(f'{x} Score')\n",
        "    ax.set_ylabel(f'{y} Score')\n",
        "    ax.grid(True)  # Add grid for better readability\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the complete figure with all subplots\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6jac_-DGclD"
      },
      "source": [
        "Sentiment results from VADER using minimally processed and preprocessed text are comparable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x69S7eodvJf_"
      },
      "source": [
        "### TextBlob\n",
        "\n",
        "TextBlob is a Python library for processing textual data. It provides a simple API for common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, projectification, translation, and more.\n",
        "\n",
        "**Key Features of TextBlob in Sentiment Analysis:**\n",
        "\n",
        "* **Sentiment Analysis Approach:** TextBlob uses a lexicon-based approach, specifically, the pattern library, for sentiment analysis. It calculates sentiment by assigning polarity (positive or negative) and subjectivity scores (objective or subjective) to text.\n",
        "* **Polarity and Subjectivity:** It provides two measures:\n",
        "  * **Polarity:** A float within the range [-1.0, 1.0] where 1 means positive sentiment and -1 means a negative sentiment.\n",
        "  * **Subjectivity:** A float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
        "\n",
        "TextBlob assigns polarity score to each word and then averages them to find the document level polarity score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS34y2nA2MPi"
      },
      "outputs": [],
      "source": [
        "#Apply to preprocessed data\n",
        "\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Ensure you've installed TextBlob\n",
        "!pip install textblob\n",
        "\n",
        "# Assuming df['normalized_narrative'] contains lists of words as your normalized text data\n",
        "# Convert lists of words back into strings if needed\n",
        "df['text_string'] = df['normalized_narrative'].apply(' '.join)\n",
        "\n",
        "# Define a function to analyze sentiment and extract polarity and subjectivity\n",
        "def add_sentiment_columns(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "\n",
        "# Apply the function and create new columns for polarity and subjectivity\n",
        "df['ppolarity'], df['psubjectivity'] = zip(*df['text_string'].apply(add_sentiment_columns))\n",
        "\n",
        "# Example to display the new columns\n",
        "print(df[['text_string', 'ppolarity', 'psubjectivity']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSyWeBwmVK0Y"
      },
      "outputs": [],
      "source": [
        "df['normalized_narrative'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GlVo8vw86Up"
      },
      "source": [
        "### SentiWordNet\n",
        "SentiWordNet is an enhancement of the popular lexical database for the English language, WordNet, which extends it with sentiment information. Specifically, it assigns to each synset,  \"synonym set,\" a group of words that are synonymous within a specific context, each expressing a particular concept or idea, of WordNet three sentiment scores: positivity, negativity, and objectivity.\n",
        "\n",
        "**How It Works:**\n",
        "SentiWordNet uses synsets from WordNet, which are sets of cognitive synonyms, each expressing a distinct concept. Synsets are linked by conceptual-semantic and lexical relations. SentiWordNet adds to each synset:\n",
        "\n",
        "* **Positive Score:** How positive the meanings of the words in the synset are.\n",
        "* **Negative Score:** How negative the meanings are.\n",
        "Objective Score: How objective or neutral the meanings are.\n",
        "\n",
        "In SentiWordNet, each synset is supposed to have three sentiment scores: positivity, negativity, and objectivity, with the sum of these three scores generally intended to be equal to 1. Then, for each document, sentiscore is calculated by adding the positive and negative scores.\n",
        "\n",
        "**Part-of-Speech (POS) Tagging**\n",
        "\n",
        "POS tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on its definition and context. Some sentiment analyzers utilize POS tagging to assign polarity scores. Common POS tags include:\n",
        "\n",
        "* Noun (NN, NNS, NNP, NNPS)\n",
        "* Verb (VB, VBD, VBG, VBN, VBP, VBZ)\n",
        "* Adjective (JJ, JJR, JJS)\n",
        "* Adverb (RB, RBR, RBS)\n",
        "\n",
        "Both TextBlob and SentiWordNet use POS tagging. However, TextBlob has a built-in POS tagger whereas SentiWordNet does not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPU--fgLEzxW"
      },
      "outputs": [],
      "source": [
        "#takes approx 17 mins\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure you have the necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Assuming df is your DataFrame and it already contains 'normalized_narrative'\n",
        "# Initialize an empty list to hold POS-tagged tokens\n",
        "postagging = []\n",
        "\n",
        "# POS tagging the normalized tokens\n",
        "for tokens in df['normalized_narrative']:\n",
        "    postagging.append(nltk.pos_tag(tokens)) #nltk.pos_tag(tokens): function from nltk that takes tokens and returns a tuple of word and POS tag ('quick','JJ')\n",
        "\n",
        "df['pos_tags'] = postagging #stores list of pos-tagged tokens in a column in df\n",
        "\n",
        "#NLTK library uses the Penn Treebank tagset whereas WorNet uses a different one. This block of code converts penn to wordnet tags.\n",
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
        "def get_sentiment(word, tag):\n",
        "    wn_tag = penn_to_wn(tag) #convert penn tag to wordnet tag\n",
        "\n",
        "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "        return []\n",
        "\n",
        "    # Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet.\n",
        "    # Synset instances are the groupings of synonymous words that express the same concept.\n",
        "    # Some of the words have only one Synset and some have several.\n",
        "    synsets = wn.synsets(word, pos=wn_tag) # Retrieve synsets for the word with the given POS tag\n",
        "    if not synsets:\n",
        "        return []\n",
        "\n",
        "    # Take the first sense, the most common (most likely sense of the word)\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name()) #retrieves the corresponding SentiWordNet synset for the given WordNet synset., synset.name(): unique identifier for synset in wordnet,\n",
        "\n",
        "    return [synset.name(), swn_synset.pos_score(), swn_synset.neg_score(), swn_synset.obj_score()] #returns synset name, positive, negative, and objective score\n",
        "\n",
        "# Initialize sentiment scores list\n",
        "senti_score = []\n",
        "\n",
        "# Calculate sentiment scores\n",
        "for pos_val in df['pos_tags']: #iterate over pos_tag pairs in each row\n",
        "    pos, neg = 0, 0 #initialize pos and neg score\n",
        "    senti_val = [get_sentiment(x, y) for (x, y) in pos_val]\n",
        "    for score in senti_val:\n",
        "        try:\n",
        "            pos += score[1]  # positive score is stored at 2nd position\n",
        "            neg += score[2]  # negative score is stored at 3rd position\n",
        "        except:\n",
        "            continue\n",
        "    senti_score.append(pos - neg)\n",
        "\n",
        "# Add sentiment scores to DataFrame\n",
        "df['senti_score'] = senti_score\n",
        "\n",
        "# Display the results\n",
        "print(df[['Company', 'normalized_narrative','pos_tags', 'senti_score']])\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHNsZuTgWIVr"
      },
      "outputs": [],
      "source": [
        "# Print the range of senti_score since senti scores are just added together\n",
        "print(f\"Minimum senti_score: {df['senti_score'].min()}\")\n",
        "print(f\"Maximum senti_score: {df['senti_score'].max()}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histogram of sentiment scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['senti_score'], bins=20, edgecolor='k')\n",
        "plt.title('Distribution of Sentiment Scores')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Display examples with highest and lowest sentiment scores\n",
        "print(\"Examples with highest sentiment scores:\")\n",
        "print(df.nlargest(5, 'senti_score')[['Company', 'normalized_narrative', 'senti_score']])\n",
        "print(\"\\nExamples with lowest sentiment scores:\")\n",
        "print(df.nsmallest(5, 'senti_score')[['Company', 'normalized_narrative', 'senti_score']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRy9nWnxLMO7"
      },
      "outputs": [],
      "source": [
        "#As shown in the plot above, the SentiWordNet results exceed 1. To facilitate comparison with VADER and TextBlob results, these scores need to be normalized.\n",
        "# Calculate the maximum absolute value\n",
        "max_abs_score = abs(df['senti_score']).max()\n",
        "\n",
        "# Scale by the maximum absolute value to normalize the sentiwordnet scores\n",
        "df['scaled_senti_score'] = df['senti_score'].apply(lambda x: x / max_abs_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppRzQsFVUGWl"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfGZQSE-3uxW"
      },
      "source": [
        "### Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ABbCha22lid"
      },
      "outputs": [],
      "source": [
        "#this takes quite long;hence, not completed. You may learn more about it:\n",
        "#https://penscola.medium.com/building-a-sentiment-analysis-model-with-three-powerful-models-roberta-bert-and-distilbert-24165582f7a3\n",
        "#https://medium.com/@adityajethani/decoding-emotions-sentiment-analysis-with-distilbert-f7096da29274\n",
        "\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Load the pre-trained sentiment analysis pipeline with the specified model\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name, truncation=True)\n",
        "\n",
        "# Function to perform batch sentiment analysis\n",
        "def analyze_sentiment_batch(texts):\n",
        "    # Batch process texts, handling truncation\n",
        "    truncated_texts = [text[:512] for text in texts]  # Truncate each text to 512 characters\n",
        "    results = sentiment_analysis(truncated_texts)  # Process in batch\n",
        "    return results\n",
        "\n",
        "# Apply the batch processing function\n",
        "batch_size = 128  # Adjust based on your system's memory capacity\n",
        "df['Hugging Face Sentiment'] = pd.concat(\n",
        "    [pd.Series(analyze_sentiment_batch(batch)) for batch in np.array_split(df['text_string'], len(df) // batch_size + 1)]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Extract detailed sentiment information\n",
        "df['HF Label'] = df['Hugging Face Sentiment'].apply(lambda x: x['label'])\n",
        "df['HF Score'] = df['Hugging Face Sentiment'].apply(lambda x: x['score'])\n",
        "\n",
        "# Display the results\n",
        "print(df[['Company', 'tex_string', 'HF Label', 'HF Score']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozu507GNwtFj"
      },
      "outputs": [],
      "source": [
        "pip install dask[delayed] dask[dataframe]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLU_C92rwuYb"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from dask import delayed\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained sentiment analysis pipeline with the specified model\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name, truncation=True)\n",
        "\n",
        "# Function to perform batch sentiment analysis\n",
        "@delayed\n",
        "def analyze_sentiment_batch(texts):\n",
        "    # Batch process texts, handling truncation\n",
        "    truncated_texts = [text[:512] for text in texts]  # Truncate each text to 512 characters\n",
        "    results = sentiment_analysis(truncated_texts)  # Process in batch\n",
        "    return results\n",
        "\n",
        "# Convert the pandas DataFrame to a Dask DataFrame\n",
        "dask_df = dd.from_pandas(df, npartitions=10)  # Adjust npartitions based on your system's capacity\n",
        "\n",
        "# Apply the batch processing function using map_partitions\n",
        "results = dask_df.map_partitions(lambda df: analyze_sentiment_batch(df['text_string']), meta='object').compute()\n",
        "\n",
        "# Convert results back to pandas DataFrame for further processing (if needed)\n",
        "results_df = pd.concat(results).reset_index(drop=True)\n",
        "df['Hugging Face Sentiment'] = results_df\n",
        "\n",
        "# Extract detailed sentiment information\n",
        "df['HF Label'] = df['Hugging Face Sentiment'].apply(lambda x: x['label'])\n",
        "df['HF Score'] = df['Hugging Face Sentiment'].apply(lambda x: x['score'])\n",
        "\n",
        "# Display the results\n",
        "print(df[['Company', 'text_string', 'HF Label', 'HF Score']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2fj4n0XUKgu"
      },
      "source": [
        "# Comparing Sentiment Analysis Performance of VADER, TextBlob, and SentiWordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDo5If_8DWdJ"
      },
      "source": [
        "I want to examine the correlation between these scores to determine their similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1shrjiqNAk8"
      },
      "outputs": [],
      "source": [
        "# Ensure the plotting libraries are installed\n",
        "!pip install matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD8NedYP7Yru"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define the subplot grid layout\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 10))  # 2x1 grid of plots, overall figure size\n",
        "\n",
        "# Define the plot titles\n",
        "titles = [\n",
        "    'pPolarity vs MVADER Compound',\n",
        "    'pPolarity vs Scaled Senti Score'\n",
        "]\n",
        "\n",
        "# Define the axes (x and y pairs) for the plots\n",
        "data_pairs = [\n",
        "    ('ppolarity', 'MVADER Compound'),\n",
        "    ('ppolarity', 'scaled_senti_score')\n",
        "]\n",
        "\n",
        "# Loop over the axes and data pairs\n",
        "for ax, (x, y), title in zip(axes.flatten(), data_pairs, titles):\n",
        "    # Scatter plot on specific subplot axis\n",
        "    sns.scatterplot(data=df, x=x, y=y, ax=ax)\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    if df[x].notnull().all() and df[y].notnull().all():  # Ensure no null values\n",
        "        corr_coef = np.corrcoef(df[x], df[y])[0, 1]\n",
        "        # Adding title with correlation\n",
        "        ax.set_title(f'{title}\\nPearson Correlation Coefficient: {corr_coef:.2f}')\n",
        "    else:\n",
        "        ax.set_title(f'{title}\\nData not sufficient for correlation')\n",
        "\n",
        "    # Set x and y labels\n",
        "    ax.set_xlabel(f'{x} Score')\n",
        "    ax.set_ylabel(f'{y} Score')\n",
        "    ax.grid(True)  # Add grid for better readability\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the complete figure with all subplots\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibBAnLwzD0to"
      },
      "source": [
        "That does not look very good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9taYyHLsV3pr"
      },
      "source": [
        "# Majority Vote Sentiment\n",
        "\n",
        "Since the results from the three different lexicon based methods are not consistent, it might be worthwhile to use (1) a majority voting rule, (2) manually check a random sample of text as suggested by Singh (2022). You may also extend VADER's lexicon to domain specific information (Barik and Misra, 2024).\n",
        "\n",
        "Item 1 is implemented below. Note that item 1 uses VADER as default if a majority cannot be reached. To ensure that VADER is the best choice, item 2 can be implemented by\n",
        "1. Distributing a random selection of complaints among team members for manual categorization as negative, positive, or neutral,\n",
        "2. Assigning one team member to review and verify the categorizations made by another team member,\n",
        "3. Aggregating all categorizations and evaluating which toolâ€”VADER, TextBlob, or SentiWordNetâ€”provides the closest match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sxtwPmVV2co"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a function to classify sentiment based on score\n",
        "def classify_sentiment(score):\n",
        "    if score > 0:\n",
        "        return 'positive'\n",
        "    elif score < 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply the classification to each score\n",
        "df['polarity_class'] = df['ppolarity'].apply(classify_sentiment)\n",
        "df['vader_class'] = df['MVADER Compound'].apply(classify_sentiment)\n",
        "df['swn_class'] = df['scaled_senti_score'].apply(classify_sentiment)\n",
        "\n",
        "# Define a function to determine the majority sentiment or fallback to VADER\n",
        "def majority_vote(row):\n",
        "    sentiments = [row['polarity_class'], row['vader_class'], row['swn_class']] #create a list of sentiment class for all three methods\n",
        "    sentiment_counts = {'positive': sentiments.count('positive'),\n",
        "                        'negative': sentiments.count('negative'),\n",
        "                        'neutral': sentiments.count('neutral')} #count the number of positive, negative, and neutral sentimens classes; creates a dictionary of sentiment:count\n",
        "\n",
        "    # Determine if there is a clear majority\n",
        "    max_count = max(sentiment_counts.values()) #find the max value in sentiment_counts\n",
        "    if list(sentiment_counts.values()).count(max_count) == 1:  #retrieves values from sentiment_counts and count the number of max_counts and checks if it appears once.\n",
        "    #For example, {'positive': 1,'negative': 1, 'neutral': 1}: there is no majority\n",
        "        for sentiment, count in sentiment_counts.items():\n",
        "            if count == max_count: #if the particular segment is the max observed, return that sentiment\n",
        "                return sentiment\n",
        "    else:\n",
        "        # Fallback to VADER's compound score's sign\n",
        "        if row['MVADER Compound'] > 0:\n",
        "            return 'positive'\n",
        "        elif row['MVADER Compound'] < 0:\n",
        "            return 'negative'\n",
        "        else:\n",
        "            return 'neutral'\n",
        "\n",
        "# Apply majority vote logic to each row\n",
        "df['Majority_Vote'] = df.apply(majority_vote, axis=1) #axis=1 means applied to each row, axis=0 would mean applied to column\n",
        "\n",
        "# Display the results\n",
        "print(df[['Company','normalized_narrative', 'ppolarity', 'MVADER Compound', 'scaled_senti_score', 'Majority_Vote']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htoSZSX5YlZt"
      },
      "outputs": [],
      "source": [
        "#Let's compute performance metrics to determine which methodâ€”VADER, TextBlob, or SentiWordNetâ€”most closely aligns with the majority vote.\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_metrics(true, pred):\n",
        "    precision = precision_score(true, pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(true, pred, average='macro', zero_division=0)\n",
        "    f1 = f1_score(true, pred, average='macro', zero_division=0)\n",
        "    accuracy = accuracy_score(true, pred)\n",
        "    cm = confusion_matrix(true, pred)\n",
        "    return precision, recall, f1, accuracy, cm\n",
        "\n",
        "# Applying function to each method\n",
        "metrics_textblob = calculate_metrics(df['Majority_Vote'], df['polarity_class'])\n",
        "metrics_vader = calculate_metrics(df['Majority_Vote'], df['vader_class'])\n",
        "metrics_swn = calculate_metrics(df['Majority_Vote'], df['swn_class'])\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Method': ['TextBlob', 'VADER', 'SentiWordNet'],\n",
        "    'Precision': [metrics_textblob[0], metrics_vader[0], metrics_swn[0]],\n",
        "    'Recall': [metrics_textblob[1], metrics_vader[1], metrics_swn[1]],\n",
        "    'F1 Score': [metrics_textblob[2], metrics_vader[2], metrics_swn[2]],\n",
        "    'Accuracy': [metrics_textblob[3], metrics_vader[3], metrics_swn[3]],\n",
        "    'Confusion Matrix': [metrics_textblob[4], metrics_vader[4], metrics_swn[4]]\n",
        "})\n",
        "\n",
        "# Print the metrics table\n",
        "print(metrics_df[['Method', 'Precision', 'Recall', 'F1 Score', 'Accuracy']])\n",
        "\n",
        "# Plotting confusion matrices\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "sns.set(font_scale=1.2)  # Adjust to suitable font size\n",
        "for i, method in enumerate(['TextBlob', 'VADER', 'SentiWordNet']):\n",
        "    sns.heatmap(metrics_df.at[i, 'Confusion Matrix'], annot=True, fmt=\"d\", ax=ax[i], cmap='Blues')\n",
        "    ax[i].set_title(f'Confusion Matrix for {method}')\n",
        "    ax[i].set_xlabel('Predicted Labels')\n",
        "    ax[i].set_ylabel('True Labels')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt9pHMjmLWYX"
      },
      "source": [
        "# Reporting Sentiment Analysis Results\n",
        "\n",
        "It looks like VADER performs the best. Perhaps it would be okay to use just VADER results depending on what you would like to report.\n",
        "\n",
        "1. If you would like to report the intensity of negative emotion for each company, you could only report the negative sentiment from VADER. This has been done in literature when analyzing complaints.\n",
        "2. Although TextBlob did not perform the best, it also has a subjectivity score which could tell whether the complaints for a company are subjective or objective.\n",
        "3. If you would like to report robust results, you could aggregate majority vote results for each company using the method described in Yu et al. (2013) equation 1. Then, you could give a single sentiment score for each company.\n",
        "4. You could also display the sentiment of a company's complaints as a chart over time. You will have to aggregate the sentiments as dictionary in a cell or at least in a dictionary format.\n",
        "5. You could show the sentiment associated with different issues/sub-issues for each company. For example, perhaps the polarity of comments associated with \"Getting a credit card\" for one company is more negative than another. Here, you can either count the negative, positive, neutral sentiment projectes or find another way to aggregate polarity scores.\n",
        "\n",
        "You may consider other approaches; feel free to use any of the methods I've listed above or explore alternative solutions.\n",
        "\n",
        "Note that the results will be displayed on your tool grouped by company. You have to think about what you would like to display and how to aggregate results by company."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eZEiEntht1r"
      },
      "source": [
        "# Topic Modeling\n",
        "\n",
        "Topic modeling is a statistical technique that identifies themes in large text collections by grouping words into topics, commonly using algorithms like Latent Dirichlet Allocation (LDA).\n",
        "\n",
        "In this notebook, two methods are demonstrated: LDA and BERTopic.\n",
        "\n",
        "| Method    | Advantages    | Disadvantages    |\n",
        "|-------------|-------------|-------------|\n",
        "| LDA  | Creates a small number of topics <br> Easy to fine tune  | Topic names are not automatically created<br> Might be difficult to interpret |\n",
        "| BERTopic  | When topic numbers are not limited,<br> creates easily interpretable topics<br>More parameters to fine tune  | Usually creates a large number of topics<br>Difficult to meaningfully reduce topic numbers  |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rflYE_fXo0Ue"
      },
      "source": [
        "\n",
        "## Latent Dirichlet Algorithm\n",
        "LDA is a type of probabilistic topic model that assumes documents are a mixture of topics and that each word in the document is attributable to one of the document's topics. It is widely used in natural language processing to discover abstract topics within a collection of documents.  Topics are represented as the top N words with the highest probability of belonging to that particular topic.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "**Topics:** These are distributions over words. Each topic is characterized by a set of words with certain probability weights.\n",
        "\n",
        "**Document-Topic Distributions:** Each document is assumed to be generated from a mixture of topics. The proportion of each topic within a document is determined by the alpha parameter.\n",
        "\n",
        "**Word-Topic Distributions:** Each topic is a distribution over words, and words are generated from topics. The distribution is influenced by the beta (or eta) parameter.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "**Alpha (Î±):** Controls the mixture of topics in documents. A high alpha value suggests documents are composed of more topics, enhancing the mixture.\n",
        "\n",
        "**Beta (Î²):**Governs the distribution of words in topics. A higher beta means each topic is spread out over a wider variety of words.\n",
        "\n",
        "![Wankhade et al. (2022)](https://drive.google.com/uc?export=view&id=1f1MtuzqQBycPcvV9J7itqqkAg0xayw4t)\n",
        "\n",
        "Figure from Kapadia (2019)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0yy5-C9v7J-"
      },
      "source": [
        "### LDA Application to CFPB Complaints Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVwd-x8MFCvm"
      },
      "outputs": [],
      "source": [
        "# Install required libraries (27 mins)\n",
        "!pip install gensim\n",
        "\n",
        "# Import required modules\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Create a dictionary from the data, identify unique tokens and assign unique integer IDs\n",
        "dictionary = corpora.Dictionary(df['normalized_narrative'])\n",
        "\n",
        "# Filter out extremes to remove infrequent and too frequent words\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000) #remove words with frequency fewer than 15, remove words that appear more than 50% of documents, keep dictionary to top 100000 words\n",
        "\n",
        "# Convert document into the bag-of-words (BoW) format = list of (token_id, token_count)\n",
        "corpus = [dictionary.doc2bow(text) for text in df['normalized_narrative']]\n",
        "\n",
        "# Set parameters for LDA\n",
        "num_topics = 10 # Adjust this to your dataset,\n",
        "passes = 20  # Number of passes through the corpus during training\n",
        "a = 0.01  # Document-topic density, the higher the more different topics in document\n",
        "b = 0.9  # Word-topic density, the higher the more words in a topic\n",
        "\n",
        "# Create an LDA model\n",
        "lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=passes, random_state=42,\n",
        "                     alpha=a,eta=b)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Save the model to disk\n",
        "model_path = '/content/drive/My Drive/lda_model.model'\n",
        "lda_model.save(model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "\n",
        "# Display the topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "# Compute C_uci coherence score\n",
        "coherence_model_uci = CoherenceModel(model=lda_model, texts=df['normalized_narrative'], dictionary=dictionary, coherence='c_v')\n",
        "coherence_uci = coherence_model_uci.get_coherence()\n",
        "print('C_v Coherence Score: ', coherence_uci)\n",
        "\n",
        "# Compute C_umass coherence score\n",
        "coherence_model_umass = CoherenceModel(model=lda_model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
        "coherence_umass = coherence_model_umass.get_coherence()\n",
        "print('C_umass Coherence Score: ', coherence_umass)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzALFS7wykoQ"
      },
      "source": [
        "### LDA Performance\n",
        "LDA is an unsupervised technique;hence, the traditional performance metrics do not usually apply.\n",
        "\n",
        "When evaluating Latent Dirichlet Allocation (LDA) results, the approach depends on the objective of the analysis:\n",
        "\n",
        "* **Predictive Ability:** If assessing the model's predictive performance is key, metrics like **perplexity** are useful. Perplexity measures how well a probability model predicts a sample and is often lower for better-performing models.\n",
        "\n",
        "* **Topic Relevance:** For analyses aimed at extracting topics meaningful to humans, Vaj (2023) suggests multiple approaches:\n",
        "  1.  **Coherence scores** assess the semantic similarity between high scoring words within each topic. Different coherence measures provide insights into various aspects of topic quality. Zvornicanin (2024) and Kapadia (2019) discuss various coherence scores. Kapadia(2019) uses C_v whereas Zvornicanin (2024) suggests using C_umass. C_umass calculates how often two words appear together in the corpus. Vaj(2023) also identify C_umass and C_v as common measures.\n",
        "    * **C_v Coherence Score**: Measures semantic similarity among topic words; higher scores indicate better semantic coherence.\n",
        "   * **UMass Coherence Score**: Evaluates word co-occurrence within documents; scores closer to zero suggest greater topic coherence.\n",
        "  2. **Visualizations** such as word clouds, bar plots, or heat maps of most important words for each topic can help understand the distribution of topics.\n",
        "  3. **Compare the results with other topic modeling methods** such as BERT-based approaches.\n",
        "  4. **Topic Interpretability** is the manual interpretation of the created topics. The goal here is to ensure that the words in each topic are coherent, meaningful, and relevant to the topic label.\n",
        "  5. **Topic labeling** involves assigning human-readable labels to each topic based on the most representative words. CFPB complaints dataset already has issue and sub-issue columns. However, these categories are selected by the individual submitting the complaint, which may lead to inaccuracies.   \n",
        "\n",
        "Vaj (2023) reports other methods of evaluating results. Not all are covered in this notebook. However, those should be applied if necessary.\n",
        "\n",
        "An example of items 1, 2, and 3 are available below. Item 4 must be done to ensure meaningful results. Additionally, Item 5 or other methods from Vaj (2023) could be implemented to further enhance the quality of the topics.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxLpELVhWVVX"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWzdnN3Wz1VJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Load or define the LDA model and associated data\n",
        "lda_model = LdaModel.load('/content/drive/My Drive/lda_model.model')\n",
        "id2word = Dictionary.load('/content/drive/My Drive/lda_model.model.id2word')\n",
        "\n",
        "corpus = [id2word.doc2bow(text) for text in df['normalized_narrative']]  # Assuming 'texts' is defined and preprocessed\n",
        "\n",
        "# Path for saving/loading LDAvis data\n",
        "num_topics = lda_model.num_topics\n",
        "LDAvis_data_filepath = os.path.join('/content/drive/My Drive', f'ldavis_prepared_{num_topics}')\n",
        "\n",
        "# Prepare or load the LDAvis visualization data\n",
        "if not os.path.exists(LDAvis_data_filepath):\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "else:\n",
        "    with open(LDAvis_data_filepath, 'rb') as f:\n",
        "        LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "# Enable visualization in Jupyter Notebooks\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.display(LDAvis_prepared)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "265AFt1XwIIx"
      },
      "source": [
        "**Intertopic distance** refers to the measure of similarity or dissimilarity between topics. In the context of MDS, it involves the calculation of distances between each pair of topics based on their distribution over the document set or their semantic similarity, which can be derived from the topic-word distributions.\n",
        "\n",
        "**Marginal topic distribution:** indicates the proportion of the corpus that is covered by that topic.\n",
        "\n",
        "The **Relevance Metric** slider in PyLDAvis is controlled by a parameter called lambda (Î»), which helps adjust the terms displayed **within each topic** based on their **frequency** and **distinctiveness**:\n",
        "\n",
        "- **Î» = 1**: Displays terms that are frequent within the topic. These terms provide a general sense of the topic's content but may not be unique, often appearing in other topics as well.\n",
        "- **Î» = 0**: Focuses on terms that are most unique to the topic, highlighting words that distinctly differentiate the topic from others.\n",
        "- **Intermediate Î» values**: Offer a balance, emphasizing terms based on both their frequency in the topic and their distinctiveness compared to the entire corpus. This allows for a nuanced exploration of topic characteristics, helping to discern not just what topics generally include but also what specifically defines them.\n",
        "\n",
        "Adjusting Î» lets users explore different balances of frequency and exclusivity, which is crucial for refining topic labels and evaluating topic quality.\n",
        "\n",
        "**Saliency** identifies terms that are important **across all topics** based on their **frequency** and **distinctiveness**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfYdB8sU4k-x"
      },
      "source": [
        "###Hyperparameter Tuning\n",
        "\n",
        "To optimize topics' coherence, LDA's hyperparameters â€” alpha (document-topic density), beta (word-topic density), and the number of topics (k) â€” can be adjusted. Ideally, one would automate this search over these hyperparameters to find the combination that yields the highest coherence score. The code below aims to do that. However, because each LDA run takes approximately 50 minutes, I did not run this code. Instead, I tried a number of values for the hyperparameters and here are the results. You could run this code which might take 10+ hours or split the work among the group members to complete it in a short amount of time.\n",
        "\n",
        "Here are the partial results from manual hyperparameter tuning with Î±=Î²=0.1. Please try other values to check whether C_v and C_umass scores improve.\n",
        "\n",
        "| Number of Topics (k) | C_v Score |\n",
        "|----------------------|-----------|\n",
        "| 9                    | 0.44      |\n",
        "| 10                   | 0.45      |\n",
        "| 11                   | 0.4419    |\n",
        "| 12                   | 0.4498    |\n",
        "| 13                   | 0.4407    |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEGxt5DWepVx"
      },
      "outputs": [],
      "source": [
        "# supporting function\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
        "\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "\n",
        "    return coherence_model_lda.get_coherence()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFgvn_Huex9d"
      },
      "outputs": [],
      "source": [
        "# this loops through alpha, eta, number of topics hyperparameters of the LDA model to find the optimal (highest coherence score)\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Create a dictionary from the data\n",
        "dictionary = corpora.Dictionary(df['normalized_narrative'])\n",
        "\n",
        "# Convert document into the bag-of-words (BoW) format = list of (token_id, token_count)\n",
        "corpus = [dictionary.doc2bow(text) for text in df['normalized_narrative']]\n",
        "\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        "# Topics range\n",
        "min_topics = 2\n",
        "max_topics = 3\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 0.32, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 0.32, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),\n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "\n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word,\n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "\n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTP2vXSe9MCD"
      },
      "source": [
        "### Saving Results to Dataframe\n",
        "I will save the topic distribution for each document (complaint) as a list in a cell in a new column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH1cNS9nlc63"
      },
      "outputs": [],
      "source": [
        "# Define a mapping from topic indices to custom topic names\n",
        "#Replace Topic Name 1 etc below with the name you gave to the topics\n",
        "topic_names = {\n",
        "    0: \"Topic Name 1\",\n",
        "    1: \"Topic Name 2\",\n",
        "    2: \"Topic Name 3\",\n",
        "    3: \"Topic Name 4\",\n",
        "    4: \"Topic Name 5\",\n",
        "    5: \"Topic Name 6\",\n",
        "    6: \"Topic Name 7\",\n",
        "    7: \"Topic Name 8\",\n",
        "    8: \"Topic Name 9\",\n",
        "    9: \"Topic Name 10\",\n",
        "}\n",
        "\n",
        "# Check the number of topics to ensure you have names for all\n",
        "num_topics = lda_model.num_topics\n",
        "assert len(topic_names) == num_topics, \"Each topic must have a corresponding name\"\n",
        "\n",
        "# Create a new column in df for topic distributions using named topics\n",
        "df['LDAtopic_distribution'] = [\n",
        "    {topic_names[topic_id]: prob for topic_id, prob in lda_model.get_document_topics(item, minimum_probability=0)}\n",
        "    for item in corpus\n",
        "]\n",
        "\n",
        "# Print the first few entries in the new column to verify\n",
        "print(df['LDAtopic_distribution'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Y5218U9B_0"
      },
      "outputs": [],
      "source": [
        "# Create a new column in df for topic distributions in dictionary format\n",
        "df['LDAtopic_distribution'] = [dict(lda_model.get_document_topics(item, minimum_probability=0)) for item in corpus]\n",
        "\n",
        "\n",
        "# Print the first few entries in the new column to verify\n",
        "print(df['LDAtopic_distribution'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4VX_vuLhy33"
      },
      "source": [
        "## BERTopic\n",
        "\n",
        "BERTopic utilizes BERT (Bidirectional Encoder Representations from Transformers) model to generate dense vector representations of text, which capture the contextual nuances and semantic relationships much more effectively than traditional bag-of-words approaches.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **Contextual Topic Identification:** By leveraging BERT embeddings, BERTopic is adept at understanding the deeper meanings of words in context, leading to more relevant and coherent topics.\n",
        "* **Dimensionality Reduction:** It uses UMAP (Uniform Manifold Approximation and Projection) to reduce the high-dimensional space of text embeddings into a more manageable form without losing significant semantic relationships.\n",
        "* **Robust Clustering:** For clustering text data, BERTopic employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise), which excels in finding clusters of varying density and is particularly effective in handling noise and outliers in the data.\n",
        "* **Dynamic Topic Modeling:** BERTopic can adjust the granularity of topics extracted based on the data, allowing for flexible and dynamic topic modeling suited to specific needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6q2wcQdiMiA"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbLLhGTHiPUv"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic #50 minutes\n",
        "docs = list(df['NoCompany Complaint'].values)\n",
        "topic_model = BERTopic()\n",
        "topics, probs = topic_model.fit_transform(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRXEHno1iRiI"
      },
      "outputs": [],
      "source": [
        "freq = topic_model.get_topic_info()\n",
        "freq.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1NacJU1wOuj"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsYUxlXYiTx5"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBpTbJ0rwSK4"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEPrxlgwwU0N"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K6gXjZ1wnHC"
      },
      "outputs": [],
      "source": [
        "#can reduce topics after the model is created\n",
        "# Reduce the number of topics\n",
        "topic_model.reduce_topics(docs, nr_topics=300)\n",
        "\n",
        "# After reducing topics, you can access the updated topics and probabilities from the BERTopic instance:\n",
        "new_topics = topic_model.get_topics()\n",
        "new_probs = topic_model.get_topic_freq()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow3Z-u0U-yek"
      },
      "outputs": [],
      "source": [
        "# Assuming new_topics is a dictionary where each topic ID maps to a list of (word, score) tuples\n",
        "for topic_id, words_scores in list(new_topics.items())[:20]:  # Display only the first 15 topics for brevity\n",
        "    print(f\"Topic ID: {topic_id}\")\n",
        "    for word, score in words_scores:\n",
        "        print(f\"  {word}: {score:.4f}\")  # Formatting score to 4 decimal places\n",
        "    print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LNALEECxKRe"
      },
      "outputs": [],
      "source": [
        "#or you can merge topics using the visualizations\n",
        "topics_to_merge = [1, 2]\n",
        "topic_model.merge_topics(docs, topics, topics_to_merge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bavtXo7yxWte"
      },
      "source": [
        "Before moving on to fine tuning, you may want to remove additional non-relevant words from your topics. For example, I see Amazon as a word in one of the topics, this does not give me any interesting information about the topics, I can remove that by adding it to the \"updated_unique_words\" list in the \"Removing Domain Specific Words\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEdBP0-3w31M"
      },
      "outputs": [],
      "source": [
        "#save the model to save time\n",
        "from bertopic import BERTopic\n",
        "topic_model = BERTopic()\n",
        "topic_model.save(\"my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8sf1pj6w7Se"
      },
      "outputs": [],
      "source": [
        "#load the model\n",
        "topic_model = BERTopic.load(\"my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmAMFuhOwDV5"
      },
      "outputs": [],
      "source": [
        "#train the model using\n",
        "from bertopic import BERTopic #50 minutes\n",
        "docs = list(df['NoCompany Complaint'].values)\n",
        "topic_model = BERTopic(nr_topics=\"auto\") #or can use nr_topics=some number here to limit the number of topics exactly to some number\n",
        "topics, probs = topic_model.fit_transform(docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd2j08dHnfez"
      },
      "source": [
        "### Fine-tuning Parameters\n",
        "There are three important parameters that can be modified for BERTopic.\n",
        "\n",
        "- **n_gram_range**: The default setting is (1,1), which outputs individual words like \"New\" and \"York\" as separate entities. To treat \"New York\" as a single entity, set this parameter to (1,2).\n",
        "\n",
        "- **umap_model**: UMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique often used to visualize complex, high-dimensional data. It aims to preserve the original structure of the data while representing it in a lower-dimensional space.\n",
        "\n",
        "- **hdbscan_model**: HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that identifies clusters of various shapes and sizes based on data density. It expands high-density regions into clusters and isolates noise points that do not fit into any cluster.\n",
        "\n",
        "#### UMAP Parameters (min, max, default):\n",
        "- **n_neighbors=3 (NA,NA,15)**: Determines the number of nearest neighbors UMAP uses to approximate local data structure, focusing on the three closest neighbors for each point.\n",
        "- **n_components=3 (2,100,2)**: Sets the number of dimensions in the embedded space to three, although the default is typically two.\n",
        "- **min_dist=0.05 (0, NA,0.1)**: Controls the minimum distance between points in the embedded space. Smaller values result in a more clustered embedding, while larger values result in a more even dispersal of points.\n",
        "\n",
        "#### HDBSCAN Parameters:\n",
        "- **min_cluster_size=80 (NA,NA,5)**: Defines the minimum number of points a cluster must contain; fewer points are considered noise.\n",
        "- **min_samples=40 (NA,NA,NA)**: The larger the value of min_samples you provide, the more conservative the clustering â€“ more points will be declared as noise, and clusters will be restricted to progressively more dense areas.\n",
        "- **gen_min_span_tree=True**: Instructs HDBSCAN to build a minimum spanning tree, useful for identifying subtle cluster connections.\n",
        "- **prediction_data=True**: Enables storage of detailed data like membership probabilities of points in clusters, aiding further analysis and visualization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9XPCMzuD1-f"
      },
      "source": [
        "To get meaningful results from BERTopic,\n",
        "1. Please run the fine-tune code for different values of the hyperparameters explained above.\n",
        "2. You may also remove common words that are not meaningful, e.g., Amazon, Sears.\n",
        "3. Apply the model to the text_string column which contains the appended version of the normalized_narrative column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9hrnsf7oiQg"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic #50 minutes\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,\n",
        "                        gen_min_span_tree=True,\n",
        "                        prediction_data=True)\n",
        "\n",
        "docs = list(df['NoCompany Complaint'].values)\n",
        "topic_model = BERTopic(umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    top_n_words=10,\n",
        "    language='english',\n",
        "    calculate_probabilities=True,\n",
        "    verbose=True,\n",
        "    n_gram_range=(1, 2))\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax8ZcAt8SMmG"
      },
      "outputs": [],
      "source": [
        "freq = topic_model.get_topic_info()\n",
        "freq.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cLrkoEHSXvl"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h1lZxw3Sq5c"
      },
      "outputs": [],
      "source": [
        "print(topics)\n",
        "print(probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PIwjJoTUndR"
      },
      "source": [
        "### Saving Results to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHB4VLxMVVus"
      },
      "outputs": [],
      "source": [
        "#Let's first get a print of the topics, names, and representation\n",
        "print(topic_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBLNsOYKTOUe"
      },
      "outputs": [],
      "source": [
        "#Save name and distribution of topics in each complaint to a cell as a list.\n",
        "# Fetch topic information from BERTopic\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Create a dictionary to map topic numbers to names, ensuring keys are in integer format if needed\n",
        "topic_names = {int(row['Topic']): row['Name'] for index, row in topic_info.iterrows() if row['Topic'] != -1}\n",
        "\n",
        "\n",
        "# Convert the list of probabilities for each document into a dictionary using topic names, with corrected mapping\n",
        "topic_distributions = [\n",
        "    {topic_names.get(i, f'Unknown_Topic_{i}'): prob for i, prob in enumerate(doc) if prob > 0}\n",
        "    for doc in probs\n",
        "]\n",
        "\n",
        "# Add the topic distributions to your DataFrame\n",
        "df['BERTopic_distributions'] = topic_distributions\n",
        "\n",
        "# Display a random sample of 5 entries from the DataFrame to check the 'BERTopic_distributions' column\n",
        "print(df[['BERTopic_distributions']].sample(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY25ANrDVt8I"
      },
      "outputs": [],
      "source": [
        "#print a random sample of the BERTopic_distributions column.\n",
        "sample = df[['BERTopic_distributions']].sample(5)\n",
        "for index, row in sample.iterrows():\n",
        "    print(f\"Index: {index}, Data: {row['BERTopic_distributions']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY5sPEkGUB8a"
      },
      "outputs": [],
      "source": [
        "# store topic info in a csv in case I will need to refer to it later.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Fetch topic information from BERTopic\n",
        "freq = topic_model.get_topic_info()\n",
        "# Define the path and filename\n",
        "path = '/content/drive/My Drive/Bertopic_info.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file in the specified Google Drive folder\n",
        "freq.to_csv(path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkyZMHQ9y7k5"
      },
      "source": [
        "# Reporting Topic Modeling Results\n",
        "1. You may compare LDA and BERTopic results to identify important topics.\n",
        "2. Store the document-topic distribution for each complaint in a cell. Then, aggregate these at the company level to get a company level topic distribution. You may need to use conflation.\n",
        "3. Combine topic model and sentiment analysis results. For example, you can find the sentiment level of each topic.\n",
        "4. Report top n topics for each company.\n",
        "5. Report the distribution of top n topics for each company.\n",
        "\n",
        "\n",
        "Again, there may be other methods to report these results. Do not forget you will have to aggregate these at the company level for your credit card comparison tool. Hence, you should first decide what you would like to display on your tool.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl6HL-Y-e2JQ"
      },
      "outputs": [],
      "source": [
        "#For example, you could display a heatmap of complain topic for each company\n",
        "#Not sure how useful this is.\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame loading (replace with your actual DataFrame if not already loaded)\n",
        "# df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Filter out the data for \"DISCOVER BANK\"\n",
        "discover_bank_data = df[df['Company'] == 'DISCOVER BANK']\n",
        "\n",
        "# Assuming each row in 'BERTopic_distributions' is a dictionary of topic probabilities\n",
        "# We need to convert this into a format that can be used to create a heatmap\n",
        "\n",
        "# Create a DataFrame from the topic distributions\n",
        "topic_data = pd.DataFrame(list(discover_bank_data['BERTopic_distributions']))\n",
        "\n",
        "# Fill NaN values that can occur if some topics are missing in some documents\n",
        "topic_data = topic_data.fillna(0)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Creating the heatmap\n",
        "plt.figure(figsize=(12, 8))  # Adjust the size as needed\n",
        "sns.heatmap(topic_data, cmap='viridis', annot=False)  # 'annot=True' to show probability values in the heatmap\n",
        "plt.title('Heatmap of BERTopic Distributions for DISCOVER BANK')\n",
        "plt.xlabel('Topics')\n",
        "plt.ylabel('Documents')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOxbTMWPXO7w"
      },
      "source": [
        "# Saving Dataframe to an Updated CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stYGXlh9XP0g"
      },
      "outputs": [],
      "source": [
        "#Let's view df before we save back to csv.\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtv8KuuQXjug"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path where the CSV will be saved\n",
        "file_path = '/content/drive/My Drive/complaints_updated.csv'  # Adjust the path according to your Drive structure\n",
        "df.to_csv(file_path, index=False)\n",
        "print(f\"File saved successfully at {file_path}\")\n",
        "\n",
        "# Optional: Load the file to verify its contents\n",
        "verify_df = pd.read_csv(file_path)\n",
        "print(verify_df.head())  # Print the first few rows of the loaded DataFrame\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjkUfwTrao8N"
      },
      "source": [
        "#References\n",
        "Barai, M. K. (2021). Sentiment analysis with TextBlob and VADER. Analytics Vidhya. Retrieved May 27, 2024, from https://www.analyticsvidhya.com/blog/2021/10/sentiment-analysis-with-textblob-and-vader/\n",
        "\n",
        "Barik, K., Misra, S.(2024) Analysis of customer reviews with an improved VADER lexicon projectifier. J Big Data 11, 10 . https://doi.org/10.1186/s40537-023-00861-x\n",
        "\n",
        "Bastani, K., Namavari, H., & Shaffer, J. (2019). Latent Dirichlet allocation (LDA) for topic modeling of the CFPB consumer complaints. Expert Systems with Applications, 127, 256-271. https://doi.org/10.1016/j.eswa.2019.03.001.\n",
        "\n",
        "Bonaccorso, G. (2018). Machine Learning Algorithms - Second Edition. Packt Publishing.\n",
        "\n",
        "Bonthu, H. (2024). Rule-Based Sentiment Analysis in Python. Analytics Vidhya. Retrieved June 11, 2024 from https://www.analyticsvidhya.com/blog/2021/06/rule-based-sentiment-analysis-in-python/#:~:text=Sentiment%20Analysis%20using%20SentiWordNet&text=It%20is%20important%20to%20obtain,synset%20and%20label%20the%20text\n",
        "\n",
        "Briggs, J. (2023). Advanced Topic Modeling with BERTopic. Pinecone. io. Retrieved June 11, 2024 from https://www.pinecone.io/learn/bertopic/\n",
        "\n",
        "David, D. (2021). NLP Tutorial: Topic Modeling in Python with BerTopic. Hackernoon. Retrieved June 11, 2024 from https://hackernoon.com/nlp-tutorial-topic-modeling-in-python-with-bertopic-372w35l9\n",
        "\n",
        "\n",
        "Distante, E. (2022). BERTopic: topic modeling as you have never seen it before. Data Reply IT | DataTech. Retrieved June 11, 2024 from https://medium.com/data-reply-it-datatech/bertopic-topic-modeling-as-you-have-never-seen-it-before-abb48bbab2b2\n",
        "\n",
        "\n",
        "Hota HS, Sharma DK, Verma N. (2021). Lexicon-based sentiment analysis using Twitter data: a case of COVID-19 outbreak in India and abroad. Data Science for COVID-19. 2021:275â€“95. doi: 10.1016/B978-0-12-824536-1.00015-0. PMCID: PMC8989068.\n",
        "\n",
        "https://huggingface.co/blog/sentiment-analysis-python\n",
        "\n",
        "\n",
        "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
        "\n",
        "Kapadia, S. (2019). End-to-end topic modeling in Python: Latent Dirichlet Allocation (LDA). Towards Data Science. Retrieved May 27, 2024, from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
        "\n",
        "Korab, P. (2023). Fine-Tuning VADER Classifier with Domain-Specific Lexicons. Towards AI. Retrieved from https://pub.towardsai.net/fine-tuning-vader-projectifier-with-domain-specific-lexicons-1b23f6882f2\n",
        "\n",
        "Lee, K. (2021). Sentiment Analysis â€” Comparing 3 Common Approaches: Naive Bayes, LSTM, and VADER. Towards Data Science. Retrieved June 12, 2024 from https://towardsdatascience.com/sentiment-analysis-comparing-3-common-approaches-naive-bayes-lstm-and-vader-ab561f834f89\n",
        "\n",
        "\n",
        "Mansurova M. (2023). Topics per Class Using BERTopic:How to understand the differences in texts by categories. Towards Data Science. Retrieved June 11,2024 from https://towardsdatascience.com/topics-per-project-using-bertopic-252314f2640\n",
        "\n",
        "\n",
        "Mohamed Y. (2021). Sentiment Analysis Using Sentiwordnet. Kaggle.com. Retrieved June 11, 2024 from https://www.kaggle.com/code/yommnamohamed/sentiment-analysis-using-sentiwordnet.\n",
        "\n",
        "Nandwani, P., & Verma, R. (2021). A review on sentiment analysis and emotion detection from text. Social Network Analysis and Mining, 11(1), 81. https://doi.org/10.1007/s13278-021-00776-6\n",
        "\n",
        "\n",
        "Osman, S. M. I., & Sabit, A. (2021). Bank scandal and customer sentiment. *Preprint submitted to Elsevier*. Retrieved from https://ssrn.com/abstract=4035168\n",
        "\n",
        "Singh, A., Saha, S., Hasanuzzaman, M. et al. Multitask Learning for Complaint Identification and Sentiment Analysis. Cogn Comput 14, 212â€“227 (2022). https://doi.org/10.1007/s12559-021-09844-7\n",
        "\n",
        "Stack Exchange. (n.d.). How to properly perform sentiment analysis. Data Science Stack Exchange. Retrieved May 27, 2024, from https://datascience.stackexchange.com/questions/104794/how-to-properly-perform-sentiment-analysis\n",
        "\n",
        "SydneyF. (2020).Getting to the Point with Topic Modeling | Part 3 - Interpreting the Visualization. Alteryx.com. Retrieved June 11, 2024 from https://community.alteryx.com/t5/Data-Science/Getting-to-the-Point-with-Topic-Modeling-Part-3-Interpreting-the/ba-p/614992\n",
        "\n",
        "Qi, Y., & Shabrina, Z. (2023). Sentiment analysis using Twitter data: A comparative application of lexicon- and machine-learning-based approach. Social Network Analysis and Mining, 13(1), 31. https://doi.org/10.1007/s13278-023-01030-x\n",
        "\n",
        "Vay, T. (2023). How to evaluate a novel topic modeling method. Vtiya. Retrieved June 11, 2024 from https://vtiya.medium.com/how-to-evaluate-novel-topic-modeling-method-104ad9684428\n",
        "\n",
        "\n",
        "Wankhade, M., Rao, A. C. S., & Kulkarni, C. (2022). A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence Review, 55(3), 5731-5780. https://doi.org/10.1007/s10462-022-10144-1\n",
        "\n",
        "Yu, Y., Duan, W., & Cao, Q. (2013). The impact of social and conventional media on firm equity value: A sentiment analysis approach. *Decision Support Systems, 55*(3), 919-926. https://doi.org/10.1016/j.dss.2012.12.028\n",
        "\n",
        "Zvornicanin, E. (2024). When Coherence Score Is Good or Bad in Topic Modeling? Baeldung. Retrieved June 11, 2024 from https://www.baeldung.com/cs/topic-modeling-coherence-score\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}